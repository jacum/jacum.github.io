var store = [{
        "title": "Observability of a reactive system: Introducing Akka Sensors",
        "excerpt":"Runnables  Any reactive system, by definition, wants to be responsive, resilient and elastic, and therefore can’t be anything but message-driven.  Any action within such system is an asynchronous message that needs to be processed.  This takes a form of a Runnable: data (message) + a snippet of code that needs to be executed to process the data.   Dispatchers   Therefore, at the heart of any reactive system, there is one or more execution contexts. An execution context, backed by some sort of worker pool, accepts the Runnable in its inbound queue.  When one of the context workers becomes available, the Runnable is executed.   In Akka runtime, a Runnable could be actor mailbox, processing one message, or just ExecutionContext that handles futures or IO monad materialisation.   It is important to keep the number of worker to a healthy minimum: keeping too many idle workers incurs overhead.  Assigning them to CPU cores and removing one worker to assign another involves core context switch and is even more expensive. Hence, in an optimised low-latency reactive system, limited number of workers are occupying the cores all the time, without switching,  and just keep processing Runnables as they come.   To illustrate The following ‘supermarket’ metaphor could be applied.    (image courtesy of Freepik)      Runnables are customers   worker threads are the cashiers   CPU cores are the cash registers   If you are efficient supermarket manager, you want your customers to spend as little time in a queue as possible.   You could achieve that by opening many cash registers, but these are expensive and having them idle wastes money. So instead, you just open few of them and watch the following parameters:     how much time a customer waited in a queue before being served? (queue time)   how much time a customer spent being served by cashier? (run time)   how many workers are actually occupied serving customers? (active workers)   By tuning down the number of the workers to the necessary minimum, optimal latency is achieved.   There also can be situations when in an otherwise balanced shop, one customer is having issues with her payment and needs to call her husband to top the card balance. This blocks the cashier - he can’t do anything but wait - and all customers behind her, until she could pay (as you may have guessed, in a reactive system, it is a Runnable that behaves non-reactively, blocking thread for I/O or a mutex/lock). We should’t call the police yet (kill the thread), but a note must be made of that customer and her behaviour.  By tracking such ‘misbehaving’ customers and correcting them, we could ensure that our cashiers are never blocked.   Actors   In a reactive system, actors are stateful entities, forming transaction boundaries and guarding state integrity.   To understand the behaviour of the system under load, the following characteristics of an actor (a class of actors or any other group) must be monitored:     how many of a given actor (class) instances are active in memory and for how long? how often are actors passivated (receive timeout triggered)?   how much traffic does an actor (class) get?   how much time does an actor (class) spend on processing a message?   how many errors and unprocessed messages are there?   In addition, for persistent actors, that are recovered from durable storage:     how many recovery events were replayed to recover actor (class) and how much time did it take?   how much time a persistence actor (class) spends persisting an event?   This is essential information, that will allow engineers identify poor performance, excess resource use and root causes of incidents.   Existing solutions for Akka   Akka itself is free open source software at its core (Apache license), but to use the great Cinnamon telemetry library, commercial Lightbend subscription is required, which may not be affordable for every team.   As alternative, there is a metrics library called Kamon, using attached agent and bytecode instrumentation, to expose Akka metrics. Kamon is suitable for testing and moderate loads, but by nature of its instrumentation, it adds too much overhead that can’t be acceptable in high-load production environments.   Teams working with Akka usually chose to implement their own custom metrics for Akka in production.   Akka Sensors   Akka Sensors is a new free open source (MIT) Scala library that instruments JVM and Akka explicitly, using native Prometheus collectors, with very low overhead, for high-load production use.   The sources are published on Github.   It is a greenfield implementation, not based on either Cinnamon or Kamon.   While the library itself is new as a package, the approaches and techniques applied are distilled from many years of production experience, implementing ad-hoc custom Akka/Prometheus metrics development, and from some OSS projects.   Key measurements performed on running actors, Akka dispatchers, Cassandra client. An optional ‘runnable watcher’, configurable per dispatcher, keeps an eye on runnables, reporting stack traces of those rogue ones, hanging too long on non-reactive activities: e.g. waiting for locks, or doing blocking I/O.   Collected metrics are indeed not as extensive as with Cinnamon, at the moment, most notably lacking the automatic instrumentation for cluster/sharding and remote traffic between cluster nodes.   It comes with set of pre-configured Grafana boards and example application (http4s API + Akka + Cassandra).   Actor dashboard:    Dispatcher dashboard:    References   Akka  Akka appeared in 2009. At its core is an implementation of actor model, as known from Erlang, rewritten in Scala. Actors are stateful entities, communicating with each other asynchronously, by passing messages around. Each actor is guaranteed to process just one message a time, allowing for lock-free mutable state updates.   On top of actors keeping their state in memory, there is Akka Persistence, adding robust event sourcing, and Akka Cluster with Sharding to distribute persistent actor on available cluster nodes. Backed by scalable database such as Cassandra and scalable streaming such as Kafka, the result is a platform for nearly-infinite scalable system.   It took few years to mature into industrial quality software, and now Akka is being successfully used highly concurrent event processing systems across wide variety of industries: from gambling to banking, and from postal logistics to IoT - where each millisecond in latency matters, and data is extremely valuable. Scaling such a system to process 10x times the current load is solved by adding hardware, but, generally, without rewriting any code.   Prometheus   Prometheus is free open source (Apache) time-series database that is widely used to keep process metrics.   Prometheus collectors for JVM could be enhanced with any kind of metrics to collect, using very low-overhead concurrent JVM primitives.  ","categories": [],
        "tags": ["akka"],
        "url": "/akka-sensors/",
        "teaser": null
      },{
        "title": "X Platform: applications, sharing state among multiple users in real time",
        "excerpt":"A high-level conceptual definition of a platform for stateful event processing with low latency is given.      Features     Client connections            Optimised for low latency       Client/server latency measured and proactively monitored on all levels - also on the client for better UX       Connections on edge servers (smooth rollouts of updates, geo-expansion)       Two-way resilient communication between end user devices and the entities, persistent socket       The users perceive state change as immediate: In system message roundtrip p99 delays &lt;100ms           Stateful entities            Large entities shared to 1000+ users, 1000+ updates per second.       Small entities (e.g. user states) 1M+ active users       Local platform states - strong consistency, CQRS/event sourcing for transactional integrity       Remote state proxies - eventual consistency, strong consistency via sync/remote system call       Durability: entity (inclusing all per-user states) is guaranteed to be recovered in &lt;1 second, even in case of severe failure.       Scalability: entities are independent, not locking each other resources, smooth horisontal upscaling by adding nodes       Caching: read models with tunable consistency, downstream aggregation           Message transport            ‘At most once’ delivery for messages between state nodes and edge nodes       Guarantees uniqueness of stateful entity receiving the message       Parallelisation and scaling of traffic           Development and delivery            Sandbox model for functionality development and testing (pure domain-leve APIs for events, client messages)       Continuous delivery with minimal friction       Rolling updates: applications are updated without any noticeable downtime           Extensible architecture:            User identity via JWT tokens       Clearly defined and well maintained APIs       Downstreaming events for live views, reporting, backoffices…           Edge and state nodes   Nodes:     Edge nodes are the nodes to which end user devices (phones, browsers) are connected.   State nodes keep the actual state, as identifiable entities, in memory and as durable copy.   Edges could be public or internal:     Public edge nodes can be located geographically close to the end users.   Internal edge nodes are located on premises, connecting special interfaces and devices with the system.  They could use either pull or push, any kind of API that the device needs. Authentication on the internal nodes is done by IP address or some other hard-wired way.   State nodes’ location may be subject to regulations (where all stuff happens).   Transport layer connects edge nodes with state nodes. Each edge or state node could be brought down gracefully transparently for the users, without any real-time process being disrupted.   For development purposes, local/minimal transport is supported, to allow development and running functional tests in the same process.   Event sourced states   Event sourced, dynamically updated states are the core business of the platform.   CIA levels:     confidentiality - only authorised users could access states   integrity - the state is at all times consistent   availability - the state is either available, or being recovered after switching from leaving node   Categories and IDs  By the nature of the objects they represent, stateful entities appear in several categories. Each category’s states differ in size (hundreds of bytes to some megabytes), the numbers of entities within category (1 (singleton), &lt;1000 or 1M+&gt;?) and how often they are updated/queried.   Each entity has an ID unique within its category.   Each state node can hold entities from multiple categories, the entity distribution layout (which node holds which share of which categories) could be configured.   There may be dedicated ‘lanes’ in the transport layer - to allow tuning for QoS, etc. per class of communication/category.   Event sourcing   State entities are implemented using CQRS/event sourcing paradigm. An entity processes one message at a time, decides whether or not it changes the state. If it does, an event is generated and persisted to durable storage. Then change is applied to the state. Finally, some messages could be sent back to the clients.   API for a state entity includes logics for standard client messages like connect and terminate(d), persistence journal, and outbound communication.   All events, that change the state, are streamed down to events topic as part of state entity API, for secondary aggregation/display or offline reporting needs.   The stream of events of an entity is also published for downstream consumers and bulding of read models (caches) across the platform. The platform API also supports creation of mirror entities, feeding on the stream of events from events topic and recovering the exact state.   Durability and recovery  Event-sourced states uses event storage, durable database persisting state on physical media:     To save the stream of events   For eventual state recovery (snapshots + events)   It is of vital importance to ensure instant recovery of the entities. If recovery from the primary persistence is not enough, an optimisation could be applied: at the cost of extra memory, each state node assigned to the domain, listens to events topic and keeps the mirrored state in memory. When the state needs to be recovered, the readily available state could be picked up from the memory immediately, without recovering it from persistent storage.   State proxies   To query and update state that is external to the platform, state proxies are used. By definition, it is not strongly consistent, unless explicitly requested from external primary source. For many uses, ‘last known good’ state may be sufficient, so the state proxy holds last known remote state and its timestamp.   The state proxies support the following operations:     push state from primary (incoming update)   force pull state from primary   send state update command + force pull updated state (one atomic operation)   read cached ‘last known’ state   Message transport   Message transport delivers messages from/to clients on the edges, and between stateful entities.   It guarantees that the messages will be delivered to an entity on a given node, one at a time, and there will be just one such instance, per entity ID.   Per message, delivery guarantee - at most once, with duplicates possible. The entities are responsible for idempotent message processing - assuming each message has unique ID, generated by its sender.   It delivers messages from the clients to the state entities (inbound traffic) and from entities to the clients (outbound). It also delivers messages between the states.   Inbound traffic   Edge node uses entity ID as a key to guarantee that all these IDs are coming through the same partition, preserving the sequence.   To save on latency, edge nodes do not parse the payload of the actual messages, passing it through, just using an envelope (entity ID as the key and adding connection ID and edge node own timestamp as attributes).   Message transport guarantees to deliver messages addressed to an instance entity with certain ID, ensuring it is one and only one instance at all available state nodes. This prevents split-brain scenarios in which different states under the same ID exist.   It is up to the state entity to process the message and ensure that it is valid and authorised, according to connection ID (JWT claims were sent in connect when the connection was established) and edge node timestamp, to apply time-related logics.   Outbound messages (from stateful entities to the clients) contain a target lookup query, that targets either one client  connection by ID or all clients connected to a certain entity, maybe or some others. This allows saving on fan-out outbound traffic, when a message needs to be delivered to many clients at the same entity. All edge nodes listen to all messages in outbound channel and use the lookups to send them to locally connected clients.   Client connections   End-user device establishes a websocket connection to the public edge node, using token issued by identity service for authorisation.   The same device could establish and operation multiple connections at the same time, possibly via different edge nodes.   Internal devices may connect to internal edges by various means, authentication is per IP address or somehow per device class. Internal edges may implement any kind of interface, e.g. HTTP push, HTTP pull, websocket or UART/serial port.   Security  Websocket HTTP upgrade request carries Authorization: Bearer xxx JWT token to authorise given client for connection.   The connection, in its URL, always addresses some stateful entity, and there’s protocol in place for joining/leaving the state. Public edge node validates JWT token signature, and passes claims of the token are sent to the state entity in the initial connect message. The entity either accepts the client and sends state update message, or sends terminate if the claims are not accepted. When connection is dropped by client, disconnect message is sent to the entity, that could schedule handling of disconnect or wait till eventual reconnect.   The claims in the token may contain specific end user roles, so the stateful entity knows who of the connected users is entitled to what.   Attributes  Each connection has the following attributes:     connection ID - to track zombie connections/refreshes   entity domain (uses same communication channel)   domain entity ID (aggregate root within a domain)   Connection ID  Connection ID is generated as UUID by the client. Node doesn’t care about the ID content and assumes it’s just an unique string, long enough to prevent “insecure direct object reference” kind of guess attack. When a connection is dropped, client is required to reconnect keeping the same connection ID.   Termination  Edge node, on request of the state entity or otherwise, can send terminate message requesting end user device to close connection and either reconnect or go away. In particular, client connection is terminated upon session timeout or a forced logout. When client just goes away and closes the connection (in fact, when any kind of socket exception kills the connection), terminated message is sent to the state entity. It is up to state entity to wait if the client comes back, ‘remembering’ him for some grace period, hoping that he comes back after a while, with same connection ID, applying any kind of custom ‘cleanup’ logics.   ","categories": [],
        "tags": ["concept"],
        "url": "/platform/",
        "teaser": null
      },]
